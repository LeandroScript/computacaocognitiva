{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe0a034",
      "metadata": {
        "id": "dbe0a034"
      },
      "outputs": [],
      "source": [
        "Tokenização e Segmentação de sentenças"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56ea25ed",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "39606693ba124544ad08acfe905f2f91",
            "a9de942ceab4447e814c09dd3efbb595"
          ]
        },
        "id": "56ea25ed",
        "outputId": "0a21cd60-ec24-49b2-a81d-152c3818d7e2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39606693ba124544ad08acfe905f2f91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9de942ceab4447e814c09dd3efbb595",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.0/models/tokenize/combined.pt:   0%|    …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-28 23:48:36 INFO: Loading these models for language: en (English):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | combined |\n",
            "========================\n",
            "\n",
            "2022-04-28 23:48:36 INFO: Use device: cpu\n",
            "2022-04-28 23:48:36 INFO: Loading: tokenize\n",
            "2022-04-28 23:48:36 INFO: Done loading processors!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== Sentence 1 tokens =======\n",
            "id: (1,)\ttext: This\n",
            "id: (2,)\ttext: is\n",
            "id: (3,)\ttext: a\n",
            "id: (4,)\ttext: test\n",
            "id: (5,)\ttext: sentence\n",
            "id: (6,)\ttext: for\n",
            "id: (7,)\ttext: stanza\n",
            "id: (8,)\ttext: .\n",
            "====== Sentence 2 tokens =======\n",
            "id: (1,)\ttext: This\n",
            "id: (2,)\ttext: is\n",
            "id: (3,)\ttext: another\n",
            "id: (4,)\ttext: sentence\n",
            "id: (5,)\ttext: .\n"
          ]
        }
      ],
      "source": [
        "import stanza\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize') \n",
        "doc = nlp('This is a test sentence for stanza. This is another sentence.') \n",
        "for i, sentence in enumerate(doc.sentences):     \n",
        "    print(f'====== Sentence {i+1} tokens =======')     \n",
        "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c122b95",
      "metadata": {
        "id": "0c122b95",
        "outputId": "eb10bc67-ba70-4f40-e683-653400966e50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['This is a test sentence for stanza.', 'This is another sentence.']\n"
          ]
        }
      ],
      "source": [
        "print([sentence.text for sentence in doc.sentences])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cdfbedb",
      "metadata": {
        "id": "9cdfbedb"
      },
      "outputs": [],
      "source": [
        "Tokenização sem Segmentação de Sentença"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43cd2696",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c6545888edfd40f99a331fd13f4ec3e6"
          ]
        },
        "id": "43cd2696",
        "outputId": "36a24ad9-e251-4bbe-a14a-9fe5b6c6f681"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6545888edfd40f99a331fd13f4ec3e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-28 23:51:11 INFO: Loading these models for language: en (English):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | combined |\n",
            "========================\n",
            "\n",
            "2022-04-28 23:51:11 INFO: Use device: cpu\n",
            "2022-04-28 23:51:11 INFO: Loading: tokenize\n",
            "2022-04-28 23:51:11 INFO: Done loading processors!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== Sentence 1 tokens =======\n",
            "id: (1,)\ttext: This\n",
            "id: (2,)\ttext: is\n",
            "id: (3,)\ttext: a\n",
            "id: (4,)\ttext: sentence\n",
            "id: (5,)\ttext: .\n",
            "====== Sentence 2 tokens =======\n",
            "id: (1,)\ttext: This\n",
            "id: (2,)\ttext: is\n",
            "id: (3,)\ttext: a\n",
            "id: (4,)\ttext: second\n",
            "id: (5,)\ttext: .\n",
            "id: (6,)\ttext: This\n",
            "id: (7,)\ttext: is\n",
            "id: (8,)\ttext: a\n",
            "id: (9,)\ttext: third\n",
            "id: (10,)\ttext: .\n"
          ]
        }
      ],
      "source": [
        "import stanza\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_no_ssplit=True)\n",
        "doc = nlp('This is a sentence.\\n\\nThis is a second. This is a third.')\n",
        "for i, sentence in enumerate(doc.sentences):\n",
        "    print(f'====== Sentence {i+1} tokens =======')\n",
        "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b5c630f",
      "metadata": {
        "id": "0b5c630f"
      },
      "outputs": [],
      "source": [
        "Processar texto pré-tokenizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9e60dd0",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c80998aa7b6d4286ad365dc4500adc0b"
          ]
        },
        "id": "d9e60dd0",
        "outputId": "2df88db0-0232-4e5d-84b8-b99051338459"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c80998aa7b6d4286ad365dc4500adc0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-28 23:52:58 INFO: Loading these models for language: en (English):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | combined |\n",
            "========================\n",
            "\n",
            "2022-04-28 23:52:58 INFO: Use device: cpu\n",
            "2022-04-28 23:52:58 INFO: Loading: tokenize\n",
            "2022-04-28 23:52:58 INFO: Done loading processors!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== Sentence 1 tokens =======\n",
            "id: (1,)\ttext: This\n",
            "id: (2,)\ttext: is\n",
            "id: (3,)\ttext: token.ization\n",
            "id: (4,)\ttext: done\n",
            "id: (5,)\ttext: my\n",
            "id: (6,)\ttext: way!\n",
            "====== Sentence 2 tokens =======\n",
            "id: (1,)\ttext: Sentence\n",
            "id: (2,)\ttext: split,\n",
            "id: (3,)\ttext: too!\n"
          ]
        }
      ],
      "source": [
        "import stanza\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_pretokenized=True)\n",
        "doc = nlp('This is token.ization done my way!\\nSentence split, too!')\n",
        "for i, sentence in enumerate(doc.sentences):\n",
        "    print(f'====== Sentence {i+1} tokens =======')\n",
        "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b6ec391",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "53ee8f37ce5c4a8f8aa20376764fec6e"
          ]
        },
        "id": "2b6ec391",
        "outputId": "7e3cdb41-9832-491f-ca6a-e8725757aa0d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53ee8f37ce5c4a8f8aa20376764fec6e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-28 23:53:36 INFO: Loading these models for language: en (English):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | combined |\n",
            "========================\n",
            "\n",
            "2022-04-28 23:53:36 INFO: Use device: cpu\n",
            "2022-04-28 23:53:36 INFO: Loading: tokenize\n",
            "2022-04-28 23:53:36 INFO: Done loading processors!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== Sentence 1 tokens =======\n",
            "id: (1,)\ttext: This\n",
            "id: (2,)\ttext: is\n",
            "id: (3,)\ttext: token.ization\n",
            "id: (4,)\ttext: done\n",
            "id: (5,)\ttext: my\n",
            "id: (6,)\ttext: way!\n",
            "====== Sentence 2 tokens =======\n",
            "id: (1,)\ttext: Sentence\n",
            "id: (2,)\ttext: split,\n",
            "id: (3,)\ttext: too!\n"
          ]
        }
      ],
      "source": [
        "import stanza\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_pretokenized=True)\n",
        "doc = nlp([['This', 'is', 'token.ization', 'done', 'my', 'way!'], ['Sentence', 'split,', 'too!']])\n",
        "for i, sentence in enumerate(doc.sentences):\n",
        "    print(f'====== Sentence {i+1} tokens =======')\n",
        "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segue a saída com tokenize_pretokenized=False"
      ],
      "metadata": {
        "id": "PTKihd5qtQ8-"
      },
      "id": "PTKihd5qtQ8-"
    },
    {
      "cell_type": "code",
      "source": [
        "====== Sentence 1 tokens ======= \n",
        "id: 1   text: This \n",
        "id: 2   text: is \n",
        "id: 3   text: token \n",
        "id: 4   text: . \n",
        "id: 5   text: ization \n",
        "id: 6   text: done \n",
        "id: 7   text: my \n",
        "id: 8   text: way \n",
        "id: 9   text: ! \n",
        "====== Sentence 2 tokens ======= \n",
        "id: 1   text: Sentence \n",
        "id: 2   text: split \n",
        "id: 3   text: , \n",
        "id: 4   text: too \n",
        "id: 5   text: !"
      ],
      "metadata": {
        "id": "Hwv2oxq-tlAf"
      },
      "id": "Hwv2oxq-tlAf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa4c10a9",
      "metadata": {
        "id": "aa4c10a9"
      },
      "outputs": [],
      "source": [
        "Lematização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eafc9cd7",
      "metadata": {
        "id": "eafc9cd7"
      },
      "outputs": [],
      "source": [
        "Exemplo de utilização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa101ac3",
      "metadata": {
        "scrolled": false,
        "colab": {
          "referenced_widgets": [
            "ccd21f7fea9c442f959ed716d22a0770",
            "37b6b8ad098242d8a5d36decb8a7b987",
            "9618be63d5ff4d2abc7fc3556b2773f8",
            "8f7587e766c84066a87f562c849cf616"
          ]
        },
        "id": "fa101ac3",
        "outputId": "0581a0eb-3d71-4bc9-d7ff-8714a00d149b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccd21f7fea9c442f959ed716d22a0770",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-28 23:59:23 WARNING: Can not find mwt: default from official model list. Ignoring it.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37b6b8ad098242d8a5d36decb8a7b987",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.0/models/pos/combined.pt:   0%|         …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9618be63d5ff4d2abc7fc3556b2773f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.0/models/lemma/combined.pt:   0%|       …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f7587e766c84066a87f562c849cf616",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.0/models/pretrain/combined.pt:   0%|    …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-29 00:00:04 INFO: Loading these models for language: en (English):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | combined |\n",
            "| pos       | combined |\n",
            "| lemma     | combined |\n",
            "========================\n",
            "\n",
            "2022-04-29 00:00:04 INFO: Use device: cpu\n",
            "2022-04-29 00:00:04 INFO: Loading: tokenize\n",
            "2022-04-29 00:00:04 INFO: Loading: pos\n",
            "2022-04-29 00:00:05 INFO: Loading: lemma\n",
            "2022-04-29 00:00:05 INFO: Done loading processors!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word: Barack \tlemma: Barack\n",
            "word: Obama \tlemma: Obama\n",
            "word: was \tlemma: be\n",
            "word: born \tlemma: bear\n",
            "word: in \tlemma: in\n",
            "word: Hawaii \tlemma: Hawaii\n",
            "word: . \tlemma: .\n"
          ]
        }
      ],
      "source": [
        "import stanza\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
        "doc = nlp('Barack Obama was born in Hawaii.')\n",
        "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Tokenização, segmentação de sentenças e lematização.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}